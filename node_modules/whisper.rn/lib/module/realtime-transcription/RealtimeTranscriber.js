/* eslint-disable class-methods-use-this */

import { SliceManager } from './SliceManager';
import { WavFileWriter } from '../utils/WavFileWriter';
import { VAD_PRESETS } from './types';

/**
 * RealtimeTranscriber provides real-time audio transcription with VAD support.
 *
 * Features:
 * - Automatic slice management based on duration
 * - VAD-based speech detection and auto-slicing
 * - Configurable auto-slice mechanism that triggers on speech_end/silence events
 * - Memory management for audio slices
 * - Queue-based transcription processing
 */
export class RealtimeTranscriber {
  callbacks = {};
  isActive = false;
  isTranscribing = false;
  vadEnabled = false;
  transcriptionQueue = [];
  accumulatedData = new Uint8Array(0);
  wavFileWriter = null;

  // Simplified VAD state management
  lastSpeechDetectedTime = 0;

  // Track VAD state for proper event transitions
  lastVadState = 'silence';

  // Track last stats to emit only when changed
  lastStatsSnapshot = null;

  // Store transcription results by slice index
  transcriptionResults = new Map();

  // Store VAD events by slice index for inclusion in transcribe events
  vadEvents = new Map();
  constructor(dependencies) {
    let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
    let callbacks = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};
    this.whisperContext = dependencies.whisperContext;
    this.vadContext = dependencies.vadContext;
    this.audioStream = dependencies.audioStream;
    this.fs = dependencies.fs;
    this.callbacks = callbacks;

    // Set default options with proper types
    this.options = {
      audioSliceSec: options.audioSliceSec || 30,
      audioMinSec: options.audioMinSec || 1,
      maxSlicesInMemory: options.maxSlicesInMemory || 3,
      vadOptions: options.vadOptions || VAD_PRESETS.default,
      vadPreset: options.vadPreset,
      autoSliceOnSpeechEnd: options.autoSliceOnSpeechEnd || true,
      autoSliceThreshold: options.autoSliceThreshold || 0.5,
      transcribeOptions: options.transcribeOptions || {},
      initialPrompt: options.initialPrompt,
      promptPreviousSlices: options.promptPreviousSlices ?? true,
      audioOutputPath: options.audioOutputPath,
      logger: options.logger || (() => {})
    };

    // Apply VAD preset if specified
    if (this.options.vadPreset && VAD_PRESETS[this.options.vadPreset]) {
      this.options.vadOptions = {
        ...VAD_PRESETS[this.options.vadPreset],
        ...this.options.vadOptions
      };
    }

    // Enable VAD if context is provided and not explicitly disabled
    this.vadEnabled = !!this.vadContext;

    // Initialize managers
    this.sliceManager = new SliceManager(this.options.audioSliceSec, this.options.maxSlicesInMemory);

    // Set up audio stream callbacks
    this.audioStream.onData(this.handleAudioData.bind(this));
    this.audioStream.onError(this.handleError.bind(this));
    this.audioStream.onStatusChange(this.handleAudioStatusChange.bind(this));
  }

  /**
   * Start realtime transcription
   */
  async start() {
    if (this.isActive) {
      throw new Error('Realtime transcription is already active');
    }
    try {
      var _this$callbacks$onSta, _this$callbacks, _this$options$audioSt4, _this$options$audioSt5, _this$options$audioSt6, _this$options$audioSt7, _this$options$audioSt8;
      this.isActive = true;
      (_this$callbacks$onSta = (_this$callbacks = this.callbacks).onStatusChange) === null || _this$callbacks$onSta === void 0 ? void 0 : _this$callbacks$onSta.call(_this$callbacks, true);

      // Reset all state to ensure clean start
      this.reset();

      // Initialize WAV file writer if output path is specified
      if (this.fs && this.options.audioOutputPath) {
        var _this$options$audioSt, _this$options$audioSt2, _this$options$audioSt3;
        this.wavFileWriter = new WavFileWriter(this.fs, this.options.audioOutputPath, {
          sampleRate: ((_this$options$audioSt = this.options.audioStreamConfig) === null || _this$options$audioSt === void 0 ? void 0 : _this$options$audioSt.sampleRate) || 16000,
          channels: ((_this$options$audioSt2 = this.options.audioStreamConfig) === null || _this$options$audioSt2 === void 0 ? void 0 : _this$options$audioSt2.channels) || 1,
          bitsPerSample: ((_this$options$audioSt3 = this.options.audioStreamConfig) === null || _this$options$audioSt3 === void 0 ? void 0 : _this$options$audioSt3.bitsPerSample) || 16
        });
        await this.wavFileWriter.initialize();
      }

      // Start audio recording
      await this.audioStream.initialize({
        sampleRate: ((_this$options$audioSt4 = this.options.audioStreamConfig) === null || _this$options$audioSt4 === void 0 ? void 0 : _this$options$audioSt4.sampleRate) || 16000,
        channels: ((_this$options$audioSt5 = this.options.audioStreamConfig) === null || _this$options$audioSt5 === void 0 ? void 0 : _this$options$audioSt5.channels) || 1,
        bitsPerSample: ((_this$options$audioSt6 = this.options.audioStreamConfig) === null || _this$options$audioSt6 === void 0 ? void 0 : _this$options$audioSt6.bitsPerSample) || 16,
        audioSource: ((_this$options$audioSt7 = this.options.audioStreamConfig) === null || _this$options$audioSt7 === void 0 ? void 0 : _this$options$audioSt7.audioSource) || 6,
        bufferSize: ((_this$options$audioSt8 = this.options.audioStreamConfig) === null || _this$options$audioSt8 === void 0 ? void 0 : _this$options$audioSt8.bufferSize) || 16 * 1024
      });
      await this.audioStream.start();

      // Emit stats update for status change
      this.emitStatsUpdate('status_change');
      this.log('Realtime transcription started');
    } catch (error) {
      var _this$callbacks$onSta2, _this$callbacks2;
      this.isActive = false;
      (_this$callbacks$onSta2 = (_this$callbacks2 = this.callbacks).onStatusChange) === null || _this$callbacks$onSta2 === void 0 ? void 0 : _this$callbacks$onSta2.call(_this$callbacks2, false);
      throw error;
    }
  }

  /**
   * Stop realtime transcription
   */
  async stop() {
    if (!this.isActive) {
      return;
    }
    try {
      var _this$callbacks$onSta3, _this$callbacks3;
      this.isActive = false;

      // Stop audio recording
      await this.audioStream.stop();

      // Process any remaining accumulated data
      if (this.accumulatedData.length > 0) {
        this.processAccumulatedDataForSliceManagement();
      }

      // Process any remaining queued transcriptions
      await this.processTranscriptionQueue();

      // Finalize WAV file
      if (this.wavFileWriter) {
        await this.wavFileWriter.finalize();
        this.wavFileWriter = null;
      }

      // Reset all state completely
      this.reset();
      (_this$callbacks$onSta3 = (_this$callbacks3 = this.callbacks).onStatusChange) === null || _this$callbacks$onSta3 === void 0 ? void 0 : _this$callbacks$onSta3.call(_this$callbacks3, false);

      // Emit stats update for status change
      this.emitStatsUpdate('status_change');
      this.log('Realtime transcription stopped');
    } catch (error) {
      this.handleError(`Stop error: ${error}`);
    }
  }

  /**
   * Handle incoming audio data from audio stream
   */
  handleAudioData(streamData) {
    if (!this.isActive) {
      return;
    }
    try {
      // Write to WAV file if enabled (convert to Uint8Array for WavFileWriter)
      if (this.wavFileWriter) {
        this.wavFileWriter.appendAudioData(streamData.data).catch(error => {
          this.log(`Failed to write audio to WAV file: ${error}`);
        });
      }

      // Always accumulate data for slice management
      this.accumulateAudioData(streamData.data);
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Audio processing error';
      this.handleError(errorMessage);
    }
  }

  /**
   * Accumulate audio data for slice management
   */
  accumulateAudioData(newData) {
    const combined = new Uint8Array(this.accumulatedData.length + newData.length);
    combined.set(this.accumulatedData);
    combined.set(new Uint8Array(newData), this.accumulatedData.length);
    this.accumulatedData = combined;

    // Process accumulated data when we have enough for slice management
    const minBufferSamples = 16000 * 1; // 1 second for slice management
    if (this.accumulatedData.length >= minBufferSamples) {
      this.processAccumulatedDataForSliceManagement();
    }
  }

  /**
   * Process accumulated audio data through SliceManager
   */
  processAccumulatedDataForSliceManagement() {
    if (this.accumulatedData.length === 0) {
      return;
    }

    // Process through slice manager directly with Uint8Array
    const result = this.sliceManager.addAudioData(this.accumulatedData);
    if (result.slice) {
      this.log(`Slice ${result.slice.index} ready (${result.slice.data.length} bytes)`);

      // Process VAD for the slice if enabled
      if (!this.isTranscribing && this.vadEnabled) {
        this.processSliceVAD(result.slice).catch(error => {
          this.handleError(`VAD processing error: ${error}`);
        });
      } else if (!this.isTranscribing) {
        // If VAD is disabled, transcribe slices as they become ready
        this.queueSliceForTranscription(result.slice).catch(error => {
          this.handleError(`Failed to queue slice for transcription: ${error}`);
        });
      } else {
        this.log(`Skipping slice ${result.slice.index} - already transcribing`);
      }
      this.emitStatsUpdate('memory_change');
    }

    // Clear accumulated data
    this.accumulatedData = new Uint8Array(0);
  }

  /**
   * Check if auto-slice should be triggered based on VAD event and timing
   */
  async checkAutoSlice(vadEvent, _slice) {
    if (!this.options.autoSliceOnSpeechEnd || !this.vadEnabled) {
      return;
    }

    // Only trigger on speech_end or silence events
    const shouldTriggerAutoSlice = vadEvent.type === 'speech_end' || vadEvent.type === 'silence';
    if (!shouldTriggerAutoSlice) {
      return;
    }

    // Get current slice info from SliceManager
    const currentSliceInfo = this.sliceManager.getCurrentSliceInfo();
    const currentSlice = this.sliceManager.getSliceByIndex(currentSliceInfo.currentSliceIndex);
    if (!currentSlice) {
      return;
    }

    // Calculate current slice duration
    const currentDuration = (Date.now() - currentSlice.startTime) / 1000; // Convert to seconds
    const targetDuration = this.options.audioSliceSec;
    const minDuration = this.options.audioMinSec;
    const autoSliceThreshold = targetDuration * this.options.autoSliceThreshold;

    // Check if conditions are met for auto-slice
    const meetsMinDuration = currentDuration >= minDuration;
    const meetsThreshold = currentDuration >= autoSliceThreshold;
    if (meetsMinDuration && meetsThreshold) {
      this.log(`Auto-slicing on ${vadEvent.type} at ${currentDuration.toFixed(1)}s ` + `(min: ${minDuration}s, threshold: ${autoSliceThreshold.toFixed(1)}s, target: ${targetDuration}s)`);

      // Force next slice
      await this.nextSlice();
    } else {
      this.log(`Auto-slice conditions not met on ${vadEvent.type}: ` + `duration=${currentDuration.toFixed(1)}s, min=${minDuration}s, threshold=${autoSliceThreshold.toFixed(1)}s ` + `(minOk=${meetsMinDuration}, thresholdOk=${meetsThreshold})`);
    }
  }

  /**
   * Process VAD for a completed slice
   */
  async processSliceVAD(slice) {
    try {
      var _this$callbacks$onVad, _this$callbacks4;
      // Get audio data from the slice for VAD processing
      const audioData = this.sliceManager.getAudioDataForTranscription(slice.index);
      if (!audioData) {
        this.log(`No audio data available for VAD processing of slice ${slice.index}`);
        return;
      }

      // Convert base64 back to Uint8Array for VAD processing

      // Detect speech in the slice
      const vadEvent = await this.detectSpeech(audioData, slice.index);
      vadEvent.timestamp = Date.now();

      // Store VAD event for inclusion in transcribe event
      this.vadEvents.set(slice.index, vadEvent);

      // Emit VAD event
      (_this$callbacks$onVad = (_this$callbacks4 = this.callbacks).onVad) === null || _this$callbacks$onVad === void 0 ? void 0 : _this$callbacks$onVad.call(_this$callbacks4, vadEvent);

      // Check if auto-slice should be triggered
      await this.checkAutoSlice(vadEvent, slice);

      // Check if speech was detected and if we should transcribe
      const isSpeech = vadEvent.type === 'speech_start' || vadEvent.type === 'speech_continue';
      const isSpeechEnd = vadEvent.type === 'speech_end';
      if (isSpeech) {
        const minDuration = this.options.audioMinSec;
        // Check minimum duration requirement
        const speechDuration = slice.data.length / 16000 / 2; // Convert bytes to seconds (16kHz, 16-bit)

        if (speechDuration >= minDuration) {
          this.log(`Speech detected in slice ${slice.index}, queueing for transcription`);
          await this.queueSliceForTranscription(slice);
        } else {
          this.log(`Speech too short in slice ${slice.index} (${speechDuration.toFixed(2)}s < ${minDuration}s), skipping`);
        }
      } else if (isSpeechEnd) {
        this.log(`Speech ended in slice ${slice.index}`);
        // For speech_end events, we might want to queue the slice for transcription
        // to capture the final part of the speech segment
        const speechDuration = slice.data.length / 16000 / 2; // Convert bytes to seconds
        const minDuration = this.options.audioMinSec;
        if (speechDuration >= minDuration) {
          this.log(`Speech end detected in slice ${slice.index}, queueing final segment for transcription`);
          await this.queueSliceForTranscription(slice);
        } else {
          this.log(`Speech end segment too short in slice ${slice.index} (${speechDuration.toFixed(2)}s < ${minDuration}s), skipping`);
        }
      } else {
        this.log(`No speech detected in slice ${slice.index}`);
      }

      // Emit stats update for VAD change
      this.emitStatsUpdate('vad_change');
    } catch (error) {
      this.handleError(`VAD processing error for slice ${slice.index}: ${error}`);
    }
  }

  /**
   * Queue a slice for transcription
   */
  async queueSliceForTranscription(slice) {
    try {
      // Get audio data from the slice
      const audioData = this.sliceManager.getAudioDataForTranscription(slice.index);
      if (!audioData) {
        this.log(`No audio data available for slice ${slice.index}`);
        return;
      }
      if (this.callbacks.onBeginTranscribe) {
        const shouldTranscribe = (await this.callbacks.onBeginTranscribe({
          sliceIndex: slice.index,
          audioData,
          duration: slice.data.length / 16000 / 2 * 1000,
          // Convert to milliseconds
          vadEvent: this.vadEvents.get(slice.index)
        })) ?? true;
        if (!shouldTranscribe) {
          this.log(`User callback declined transcription for slice ${slice.index}`);
          return;
        }
      }

      // Add to transcription queue
      this.transcriptionQueue.unshift({
        sliceIndex: slice.index,
        audioData
      });
      this.log(`Queued slice ${slice.index} for transcription (${slice.data.length} samples)`);
      await this.processTranscriptionQueue();
    } catch (error) {
      this.handleError(`Failed to queue slice for transcription: ${error}`);
    }
  }

  /**
   * Detect speech using VAD context
   */
  async detectSpeech(audioData, sliceIndex) {
    if (!this.vadContext) {
      // When no VAD context is available, assume speech is always detected
      // but still follow the state machine pattern
      const currentTimestamp = Date.now();

      // Assume speech is always detected when no VAD context
      const vadEventType = this.lastVadState === 'silence' ? 'speech_start' : 'speech_continue';

      // Update VAD state
      this.lastVadState = 'speech';
      const {
        sampleRate = 16000
      } = this.options.audioStreamConfig || {};
      return {
        type: vadEventType,
        lastSpeechDetectedTime: 0,
        timestamp: currentTimestamp,
        confidence: 1.0,
        duration: audioData.length / sampleRate / 2,
        // Convert bytes to seconds
        sliceIndex
      };
    }
    try {
      const audioBuffer = audioData.buffer;

      // Use VAD context to detect speech segments
      const vadSegments = await this.vadContext.detectSpeechData(audioBuffer, this.options.vadOptions);

      // Calculate confidence based on speech segments
      let confidence = 0.0;
      let lastSpeechDetectedTime = 0;
      if (vadSegments && vadSegments.length > 0) {
        var _vadSegments;
        // If there are speech segments, calculate average confidence
        const totalTime = vadSegments.reduce((sum, segment) => sum + (segment.t1 - segment.t0), 0);
        const audioDuration = audioData.length / 16000 / 2; // Convert bytes to seconds
        confidence = totalTime > 0 ? Math.min(totalTime / audioDuration, 1.0) : 0.0;
        lastSpeechDetectedTime = ((_vadSegments = vadSegments[vadSegments.length - 1]) === null || _vadSegments === void 0 ? void 0 : _vadSegments.t1) || -1;
      }
      const threshold = this.options.vadOptions.threshold || 0.5;
      let isSpeech = confidence > threshold;
      const currentTimestamp = Date.now();

      // Determine VAD event type based on current and previous state
      let vadEventType;
      if (isSpeech) {
        vadEventType = this.lastVadState === 'silence' ? 'speech_start' : 'speech_continue';
        const minDuration = this.options.audioMinSec;
        // Check if this is a new speech detection (different from last detected time)
        if (lastSpeechDetectedTime === this.lastSpeechDetectedTime || (lastSpeechDetectedTime - this.lastSpeechDetectedTime) / 100 < minDuration) {
          if (this.lastVadState === 'silence') vadEventType = 'silence';
          if (this.lastVadState === 'speech') vadEventType = 'speech_end';
          isSpeech = false;
          confidence = 0.0;
        }
        this.lastSpeechDetectedTime = lastSpeechDetectedTime;
      } else {
        vadEventType = this.lastVadState === 'speech' ? 'speech_end' : 'silence';
      }

      // Update VAD state for next detection
      this.lastVadState = isSpeech ? 'speech' : 'silence';
      const {
        sampleRate = 16000
      } = this.options.audioStreamConfig || {};
      return {
        type: vadEventType,
        lastSpeechDetectedTime,
        timestamp: currentTimestamp,
        confidence,
        duration: audioData.length / sampleRate / 2,
        // Convert bytes to seconds
        sliceIndex,
        currentThreshold: threshold
      };
    } catch (error) {
      this.log(`VAD detection error: ${error}`);
      // Re-throw the error so it can be handled by the caller
      throw error;
    }
  }
  isProcessingTranscriptionQueue = false;

  /**
   * Process the transcription queue
   */
  async processTranscriptionQueue() {
    if (this.isProcessingTranscriptionQueue) return;
    this.isProcessingTranscriptionQueue = true;
    while (this.transcriptionQueue.length > 0) {
      const item = this.transcriptionQueue.shift();
      this.transcriptionQueue = []; // Old items are not needed anymore
      if (item) {
        // eslint-disable-next-line no-await-in-loop
        await this.processTranscription(item).catch(error => {
          this.handleError(`Transcription error: ${error}`);
        });
      }
    }
    this.isProcessingTranscriptionQueue = false;
  }

  /**
   * Build prompt from initial prompt and previous slices
   */
  buildPrompt(currentSliceIndex) {
    const promptParts = [];

    // Add initial prompt if provided
    if (this.options.initialPrompt) {
      promptParts.push(this.options.initialPrompt);
    }

    // Add previous slice results if enabled
    if (this.options.promptPreviousSlices) {
      // Get transcription results from previous slices (up to the current slice)
      const previousResults = Array.from(this.transcriptionResults.entries()).filter(_ref => {
        let [sliceIndex] = _ref;
        return sliceIndex < currentSliceIndex;
      }).sort((_ref2, _ref3) => {
        let [a] = _ref2;
        let [b] = _ref3;
        return a - b;
      }) // Sort by slice index
      .map(_ref4 => {
        var _result$transcribeEve;
        let [, result] = _ref4;
        return (_result$transcribeEve = result.transcribeEvent.data) === null || _result$transcribeEve === void 0 ? void 0 : _result$transcribeEve.result;
      }).filter(result => Boolean(result)); // Filter out empty results with type guard

      if (previousResults.length > 0) {
        promptParts.push(...previousResults);
      }
    }
    return promptParts.join(' ') || undefined;
  }

  /**
   * Process a single transcription
   */
  async processTranscription(item) {
    if (!this.isActive) {
      return;
    }
    this.isTranscribing = true;

    // Emit stats update for status change
    this.emitStatsUpdate('status_change');
    const startTime = Date.now();
    try {
      var _this$callbacks$onTra, _this$callbacks5;
      // Build prompt from initial prompt and previous slices
      const prompt = this.buildPrompt(item.sliceIndex);
      const audioBuffer = item.audioData.buffer;
      const {
        promise
      } = this.whisperContext.transcribeData(audioBuffer, {
        ...this.options.transcribeOptions,
        prompt,
        // Include the constructed prompt
        onProgress: undefined // Disable progress for realtime
      });

      const result = await promise;
      const endTime = Date.now();

      // Create transcribe event
      const {
        sampleRate = 16000
      } = this.options.audioStreamConfig || {};
      const transcribeEvent = {
        type: 'transcribe',
        sliceIndex: item.sliceIndex,
        data: result,
        isCapturing: this.audioStream.isRecording(),
        processTime: endTime - startTime,
        recordingTime: item.audioData.length / (sampleRate / 1000) / 2,
        // ms,
        memoryUsage: this.sliceManager.getMemoryUsage(),
        vadEvent: this.vadEvents.get(item.sliceIndex)
      };

      // Save transcription results
      const slice = this.sliceManager.getSliceByIndex(item.sliceIndex);
      if (slice) {
        this.transcriptionResults.set(item.sliceIndex, {
          slice: {
            // Don't keep data in the slice
            index: slice.index,
            sampleCount: slice.sampleCount,
            startTime: slice.startTime,
            endTime: slice.endTime,
            isProcessed: slice.isProcessed,
            isReleased: slice.isReleased
          },
          transcribeEvent
        });
      }

      // Emit transcribe event
      (_this$callbacks$onTra = (_this$callbacks5 = this.callbacks).onTranscribe) === null || _this$callbacks$onTra === void 0 ? void 0 : _this$callbacks$onTra.call(_this$callbacks5, transcribeEvent);
      this.vadEvents.delete(item.sliceIndex);

      // Emit stats update for memory/slice changes
      this.emitStatsUpdate('memory_change');
      this.log(`Transcribed speech segment ${item.sliceIndex}: "${result.result}"`);
    } catch (error) {
      var _this$callbacks$onTra2, _this$callbacks6;
      // Emit error event to transcribe callback
      const errorEvent = {
        type: 'error',
        sliceIndex: item.sliceIndex,
        data: undefined,
        isCapturing: this.audioStream.isRecording(),
        processTime: Date.now() - startTime,
        recordingTime: 0,
        memoryUsage: this.sliceManager.getMemoryUsage(),
        vadEvent: this.vadEvents.get(item.sliceIndex)
      };
      (_this$callbacks$onTra2 = (_this$callbacks6 = this.callbacks).onTranscribe) === null || _this$callbacks$onTra2 === void 0 ? void 0 : _this$callbacks$onTra2.call(_this$callbacks6, errorEvent);
      this.vadEvents.delete(item.sliceIndex);
      this.handleError(`Transcription failed for speech segment ${item.sliceIndex}: ${error}`);
    } finally {
      // Check if we should continue processing queue
      if (this.transcriptionQueue.length > 0) {
        await this.processTranscriptionQueue();
      } else {
        this.isTranscribing = false;
      }
    }
  }

  /**
   * Handle audio status changes
   */
  handleAudioStatusChange(isRecording) {
    this.log(`Audio recording: ${isRecording ? 'started' : 'stopped'}`);
  }

  /**
   * Handle errors from components
   */
  handleError(error) {
    var _this$callbacks$onErr, _this$callbacks7;
    this.log(`Error: ${error}`);
    (_this$callbacks$onErr = (_this$callbacks7 = this.callbacks).onError) === null || _this$callbacks$onErr === void 0 ? void 0 : _this$callbacks$onErr.call(_this$callbacks7, error);
  }

  /**
   * Update callbacks
   */
  updateCallbacks(callbacks) {
    this.callbacks = {
      ...this.callbacks,
      ...callbacks
    };
  }

  /**
   * Update VAD options dynamically
   */
  updateVadOptions(options) {
    this.options.vadOptions = {
      ...this.options.vadOptions,
      ...options
    };
  }

  /**
   * Update auto-slice options dynamically
   */
  updateAutoSliceOptions(options) {
    if (options.autoSliceOnSpeechEnd !== undefined) {
      this.options.autoSliceOnSpeechEnd = options.autoSliceOnSpeechEnd;
    }
    if (options.autoSliceThreshold !== undefined) {
      this.options.autoSliceThreshold = options.autoSliceThreshold;
    }
    this.log(`Auto-slice options updated: enabled=${this.options.autoSliceOnSpeechEnd}, threshold=${this.options.autoSliceThreshold}`);
  }

  /**
   * Get current statistics
   */
  getStatistics() {
    return {
      isActive: this.isActive,
      isTranscribing: this.isTranscribing,
      vadEnabled: this.vadEnabled,
      audioStats: {
        isRecording: this.audioStream.isRecording(),
        accumulatedSamples: this.accumulatedData.length
      },
      vadStats: this.vadEnabled ? {
        enabled: true,
        contextAvailable: !!this.vadContext,
        lastSpeechDetectedTime: this.lastSpeechDetectedTime
      } : null,
      sliceStats: this.sliceManager.getCurrentSliceInfo(),
      autoSliceConfig: {
        enabled: this.options.autoSliceOnSpeechEnd,
        threshold: this.options.autoSliceThreshold,
        targetDuration: this.options.audioSliceSec,
        minDuration: this.options.audioMinSec
      }
    };
  }

  /**
   * Get all transcription results
   */
  getTranscriptionResults() {
    return Array.from(this.transcriptionResults.values());
  }

  /**
   * Force move to the next slice, finalizing the current one regardless of capacity
   */
  async nextSlice() {
    var _this$callbacks$onTra3, _this$callbacks8;
    if (!this.isActive) {
      this.log('Cannot force next slice - transcriber is not active');
      return;
    }

    // Emit start event to indicate slice processing has started
    const startEvent = {
      type: 'start',
      sliceIndex: -1,
      // Use -1 to indicate forced slice
      data: undefined,
      isCapturing: this.audioStream.isRecording(),
      processTime: 0,
      recordingTime: 0,
      memoryUsage: this.sliceManager.getMemoryUsage()
    };
    (_this$callbacks$onTra3 = (_this$callbacks8 = this.callbacks).onTranscribe) === null || _this$callbacks$onTra3 === void 0 ? void 0 : _this$callbacks$onTra3.call(_this$callbacks8, startEvent);

    // Check if there are pending transcriptions or currently transcribing
    if (this.isTranscribing || this.transcriptionQueue.length > 0) {
      this.log('Waiting for pending transcriptions to complete before forcing next slice...');

      // Wait for current transcription queue to be processed
      await this.processTranscriptionQueue();
    }
    const result = this.sliceManager.forceNextSlice();
    if (result.slice) {
      this.log(`Forced slice ${result.slice.index} ready (${result.slice.data.length} bytes)`);

      // Process VAD for the slice if enabled
      if (!this.isTranscribing && this.vadEnabled) {
        this.processSliceVAD(result.slice).catch(error => {
          this.handleError(`VAD processing error: ${error}`);
        });
      } else if (!this.isTranscribing) {
        // If VAD is disabled, transcribe slices as they become ready
        this.queueSliceForTranscription(result.slice).catch(error => {
          this.handleError(`Failed to queue slice for transcription: ${error}`);
        });
      } else {
        this.log(`Skipping slice ${result.slice.index} - already transcribing`);
      }
      this.emitStatsUpdate('memory_change');
    } else {
      this.log('Forced next slice but no slice data to process');
    }
  }

  /**
   * Reset all components
   */
  reset() {
    this.sliceManager.reset();
    this.transcriptionQueue = [];
    this.isTranscribing = false;
    this.accumulatedData = new Uint8Array(0);

    // Reset simplified VAD state
    this.lastSpeechDetectedTime = -1;
    this.lastVadState = 'silence';

    // Reset stats snapshot for clean start
    this.lastStatsSnapshot = null;

    // Cancel WAV file writing if in progress
    if (this.wavFileWriter) {
      this.wavFileWriter.cancel().catch(error => {
        this.log(`Failed to cancel WAV file writing: ${error}`);
      });
      this.wavFileWriter = null;
    }

    // Clear transcription results
    this.transcriptionResults.clear();

    // Clear VAD events
    this.vadEvents.clear();
  }

  /**
   * Release all resources
   */
  async release() {
    var _this$wavFileWriter;
    if (this.isActive) {
      await this.stop();
    }
    await this.audioStream.release();
    await ((_this$wavFileWriter = this.wavFileWriter) === null || _this$wavFileWriter === void 0 ? void 0 : _this$wavFileWriter.finalize());
    this.vadContext = undefined;
  }

  /**
   * Emit stats update event if stats have changed significantly
   */
  emitStatsUpdate(eventType) {
    const currentStats = this.getStatistics();

    // Check if stats have changed significantly
    if (!this.lastStatsSnapshot || RealtimeTranscriber.shouldEmitStatsUpdate(currentStats, this.lastStatsSnapshot)) {
      var _this$callbacks$onSta4, _this$callbacks9;
      const statsEvent = {
        timestamp: Date.now(),
        type: eventType,
        data: currentStats
      };
      (_this$callbacks$onSta4 = (_this$callbacks9 = this.callbacks).onStatsUpdate) === null || _this$callbacks$onSta4 === void 0 ? void 0 : _this$callbacks$onSta4.call(_this$callbacks9, statsEvent);
      this.lastStatsSnapshot = {
        ...currentStats
      };
    }
  }

  /**
   * Determine if stats update should be emitted
   */
  static shouldEmitStatsUpdate(current, previous) {
    var _current$sliceStats, _current$sliceStats$m, _previous$sliceStats, _previous$sliceStats$;
    // Always emit on status changes
    if (current.isActive !== previous.isActive || current.isTranscribing !== previous.isTranscribing) {
      return true;
    }

    // Emit on significant memory changes (>10% or >5MB)
    const currentMemory = ((_current$sliceStats = current.sliceStats) === null || _current$sliceStats === void 0 ? void 0 : (_current$sliceStats$m = _current$sliceStats.memoryUsage) === null || _current$sliceStats$m === void 0 ? void 0 : _current$sliceStats$m.estimatedMB) || 0;
    const previousMemory = ((_previous$sliceStats = previous.sliceStats) === null || _previous$sliceStats === void 0 ? void 0 : (_previous$sliceStats$ = _previous$sliceStats.memoryUsage) === null || _previous$sliceStats$ === void 0 ? void 0 : _previous$sliceStats$.estimatedMB) || 0;
    const memoryDiff = Math.abs(currentMemory - previousMemory);
    if (memoryDiff > 5 || previousMemory > 0 && memoryDiff / previousMemory > 0.1) {
      return true;
    }
    return false;
  }

  /**
   * Logger function
   */
  log(message) {
    this.options.logger(`[RealtimeTranscriber] ${message}`);
  }
}
//# sourceMappingURL=RealtimeTranscriber.js.map